
# NLP

## 传统向量化方式

### 词袋模型(Bag of Words)
原理：将文本看作一个词的集合，不考虑词序和语法，只考虑词频。统计每个词在文本中出现的次数，然后将这些次数组成一个向量。
优点：简单易懂，计算速度快。
缺点：忽略了词序和语法，无法捕捉词之间的语义关系。

### 词频-逆文档频率模型(TF-IDF)
原理：在词袋模型的基础上，考虑词的权重。TF表示词频，IDF表示逆文档频率。TF-IDF = TF * IDF。TF-IDF综合考虑
词在文档中的重要性和整个语料库的常见程度。逆文档频率计算公式为：IDF = log(总文档数 / 包含该词的文档数 + 1)。
这里的总文档数是语料库中的文档总数，包含该词的文档数是包含该词的文档数。
所以逆文档频率是根据词在语料库中的分布情况来计算的，语料库是用来训练模型的数据集。
jieba库可以用来分词，并计算TF-IDF值，它用的模型是TfidfVectorizer，也就是说jieba内置了TF-IDF模型。

优点：考虑了词在文档中的重要性和整个语料库的常见程度。
缺点：忽略了词序和语法，无法捕捉词之间的语义关系。

### 词嵌入模型(Word Embedding)
原理：将每个词映射到一个低维的稠密向量空间，词向量不仅包含了词的语义信息，还捕捉了词之间的相似性关系。
其实词嵌入模型是基于神经网络的，词向量是神经网络的权重矩阵中的参数。矩阵的行数是词典的大小，矩阵的列数是词向量的维度。
维度是人为设定的，维度越高，词向量的表达能力越强，但计算复杂度也越高。这些维度包含了词的语义信息，所以词向量可以用来表示词的语义。词与词之间的相似度可以通过词向量之间的距离来计算。行数是什么意思？ 行数是词典的大小，词典是所有词的集合。

优点：考虑了词的语义信息和相似性关系。
缺点：计算复杂度高，需要大量的训练数据。
常用的词嵌入模型有Word2Vec、GloVe、FastText等。

#### 示例
假设我们有一个句子：“我爱自然语言处理”
- 词袋模型：我们可以构建一个词汇表，包括“我”、“爱”、“自然”、“语言”、“处理”。然后统计每个词在句子中出现的次数，得到一个词频向量: [1, 1, 1, 1, 1]。
- TF-IDF模型：如果“自然语言处理”这个词在语料库中比较少见，那么它的IDF值就会比较高，从而在TF-IDF向量中占据较高的权重。（IDF = log(总文档数 / 包含该词的文档数 + 1)），语料库中总文档数是固定的，包含该词的文档数是较少的，由于log函数是单调递增的，所以IDF值会比较大。
- 词嵌入模型：假设我们有一个预训练好的词向量矩阵，其中每一行对应一个词，每一列是一个词向量。我们可以将句子中的每个词映射到词向量空间，然后对这些词向量进行平均或加权平均，得到一个句子的向量表示。我们可以使用Word2Vec等工具将每个词映射到一个低维的向量空间，比如300维。然后将这些词向量取平均，得到句子的向量表示。


## 基于Transformer的向量化方式

### Transformer如何进行向量化？
1、输入嵌入（Input Embedding）
- 将输入的文本序列（每个词）转换为稠密的向量表示
- 这些向量包含了词的语义和语法信息。

2、位置编码（Positional Encoding）
- 由于Transformer不包含循环神经网络（不考虑词序），所以需要使用位置编码来表示词在句子中的位置。
- 位置编码通常是一个sin和cos的函数的组合，可以有效捕捉词在句子中的位置信息。

3、自注意力机制（Self-Attention Mechanism）
- 自注意力机制让模型能够关注到整个句子中其他相关的词，从而更好地理解整个句子的含义。
- 通过计算Query、Key和Value三个向量，模型可以计算出每个词对其他词的注意力权重。
- 加权求和得到每个词的新的表示。如何进行加权求和？ 通过softmax函数对注意力权重进行归一化，然后与Value向量相乘，最后将所有结果相加，公式：softmax(Q*K^T/sqrt(d_k)) * V。

 通过将Query向量与Key向量进行点积，并除以Key向量的维度开根号，得到注意力分数矩阵。然后，对注意力分数矩阵进行softmax归一化，将其转换为注意力概率分布。最后，将注意力概率分布与Value向量进行加权求和，即可得到每个词的新的表示。公式为：softmax(Q*K^T/sqrt(d_k)) * V。其中，Q是Query向量，K是Key向量，V是Value向量，d_k是Key向量的维度。softmax函数的作用是将注意力分数归一化，使其所有元素之和为1，从而表示每个词对当前词的重要程度。

例子：
假设我们要翻译句子“我爱北京天安门”。对于单词“北京”，我们希望知道它与其他词的关系。通过注意力机制，我们可以发现“北京”与“天安门”的相似度很高，而与“我”和“爱”的相似度较低。因此，在计算“北京”的新表示时，我们会更多地考虑“天安门”的信息。

4、多头注意力机制（Multi-Head Attention）
- 多头注意力机制通过并行计算多个自注意力层，从不同的角度捕捉信息。

5、前馈神经网络（Feed-Forward Network）
- 前馈神经网络对多头注意力机制的输出进行进一步处理，增加模型的非线性表达能力。
- 在多头注意力之后，每个位置的表示都会经过一个全连接的前馈神经网络，进一步提取特征。全连接的前馈神经网络的公式为：FFN(x) = max(0, xW_1 + b_1)W_2 + b_2。其中，x是多头注意力的输出，W_1和W_2是权重矩阵，b_1和b_2是偏置项。


6、输出层（Output Layer）
- 最终的输出可以是分类、序列标注等任务所需的向量表示。

### Transformer向量化的优势
- 并行计算：Transformer可以并行处理整个句子，而不需要像RNN那样依次处理每个词。
- 捕捉长距离依赖：通过自注意力机制，Transformer可以捕捉句子中相隔较远的词之间的依赖关系。
- 模型复杂度可控：通过调整自注意力机制的参数，可以控制模型的复杂度，从而在不同的任务中进行调整。
- 表达能力强：Transformer可以捕捉到更复杂的语言结构和语义信息。

### 基于Transformer的向量化应用
- 句子编码：将整个句子编码为一个稠密向量，可以用于文本分类、情感分析、文本相似度等任务。
- 机器翻译：将源语言句子编码为向量，然后解码为目标语言句子。
- 文本摘要：将文本编码为向量，然后生成摘要。
- 问答系统：将问题和候选答案编码为向量，计算相似度。
- 命名实体识别：将句子编码为向量，然后分类每个词的实体类型。


## Bert 模型的学习
原理：Bert 模型是基于 Transformer 的预训练语言模型，通过大规模的语料库进行预训练，学习到丰富的语言表示。
中文的Bert模型有：Bert-wwm、Bert-wwm-ext、Bert-chinese等。

使用Bert-chinese模型进行文本分类任务。
demo：https://github.com/ymcui/Chinese-BERT-wwm

首先安装依赖：
```bash
pip install torch transformers
```
然后下载预训练模型：
```bash
git lfs install
git clone https://huggingface.co/hfl/chinese-bert-wwm-ext
```

然后运行demo：
```bash
python predict.py
```


### Bert 模型的结构

### Bert 模型的预训练

### Bert 模型的微调

### Bert 模型的应用



